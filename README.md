# BackwardPropagation
Backward Propagation(backwarding/backpropagation) from the output layer to the input layer to adjust weights to minimize error/loss.

First, we need to find the derivative of total error with respect to Ooutput using the result from Forward. Then we find the derivative of Ooutput with respect to Xoutput. X_output_w is the derivative of Xoutput with respect to weight and that gives Ohidden. Derivative of total error with respect to Whidden_output equals to derivative of total error multiplied by derivative of Ooutput multiplied by derivative of Xoutput.
We randomly choose learning rate to be 0.5. To find the updated Whidden_output, we find the multiplication of alpha (learning rate) by derivative of total error with respect to hidden ouput weight, the subtract it from Whidden_output found in Forward.
Next, we find the derivative of Ohidden. Then we find derivative of total error with respect to Xhidden. Then we multiplied the two results that we just got, then multiplied it by the original input. This gives the derivative of total error with respect to Winput_hidden. Again, we use similar approach as above to update Winput_hidden by using the same value of alpha.
